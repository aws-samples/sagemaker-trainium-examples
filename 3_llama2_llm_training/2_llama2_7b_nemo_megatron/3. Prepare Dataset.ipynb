{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Data for LLM Training\n",
    "\n",
    "In this notebook we will use a dataset in Huggingface Datasets repo to train the Llama model. As part of data preparation we will do the below steps\n",
    "\n",
    "1. Download Dataset from HF hub. The notebook will use the provided dataset repo name to download it to the instance. \n",
    "2. Load and tokenize the dataset. \n",
    "3. Save the tokenized data in nemo format that can be used for training.\n",
    "\n",
    "Note: If you want to use your own dataset then you can directly provide it as jsonl file which Nemo Megatron supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the processing Job\n",
    "\n",
    "We will use SageMaker training job with an to run the data processing. We will start with importing necessary SageMaker modules from the SageMaker python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive the docker image URL stored in step 1\n",
    "%store -r docker_image \n",
    "\n",
    "use_fsx = False # set this to true and check other fsx parameters to use FSxL for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:  \n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the data processing job with NEMO neuron container\n",
    "\n",
    "We will use the custom docker image that we created in step 1 with the Neuron Image as base to run the processing Job. We will provide some of the hyperparameters according to the dataset we use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "\n",
    "hyperparameters['hf_dataset_name'] = \"wikitext\" # we will use wikitext to run this example\n",
    "hyperparameters['hf_subset'] = \"wikitext-103-v1\" # Change this depending on the dataset used.\n",
    "hyperparameters['dataset_split'] = \"train\" # Change this depending on the dataset used.\n",
    "hyperparameters['token'] = \"hf_XXXX\" # Please add your HuggingFace Token to download the gated Llama2 model.\n",
    "hyperparameters['model_id'] = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "hyperparameters['json-keys'] = 'text' # key in the dataset JSON that contains the text.\n",
    "hyperparameters['tokenizer-library'] = 'huggingface'\n",
    "hyperparameters['dataset-impl'] = 'mmap'\n",
    "hyperparameters['need-pad-id'] = \"\"\n",
    "hyperparameters['append-eod'] = \"\"\n",
    "hyperparameters['workers'] = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the FSX details from Store Magic \n",
    "\n",
    "if use_fsx:\n",
    "    #retrive fsx details\n",
    "    %store -r fsx_id\n",
    "    %store -r sec_group\n",
    "    %store -r private_subnet_id     \n",
    "    %store -r fsx_mount\n",
    "    %store -r fsx_file_system_path\n",
    "else:\n",
    "    use_fsx = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup fsx config for data channels\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "if use_fsx:\n",
    "    FS_ID = fsx_id # FSX ID\n",
    "    FS_BASE_PATH = \"/\" + fsx_mount + \"/\" + fsx_file_system_path # Path in the filesystem that needs to be mounted\n",
    "    SUBNET_ID = private_subnet_id # Subnet to launch SM jobs in\n",
    "    SEC_GRP = [sec_group]\n",
    "\n",
    "    fsx_train_input = FileSystemInput(\n",
    "        file_system_id=FS_ID,\n",
    "        file_system_type='FSxLustre',\n",
    "        directory_path=FS_BASE_PATH + \"/nemo_llama\",\n",
    "        file_system_access_mode=\"rw\"\n",
    "    )\n",
    "    hyperparameters['input'] = \"/opt/ml/input/data/train/wiki.jsonl\"\n",
    "    hyperparameters['tokenizer-type'] = '/opt/ml/input/data/train/llama7b-hf'\n",
    "    hyperparameters['output-prefix'] = '/opt/ml/input/data/train/wiki'\n",
    "    data_channels = {\"train\": fsx_train_input}\n",
    "\n",
    "else:\n",
    "    checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/nemo_llama_experiment\"\n",
    "    # we will use the sagemaker s3 checkpoints mechanism since we need read/write access to the paths.\n",
    "    hyperparameters['input'] = \"/opt/ml/checkpoints/wiki.jsonl\"\n",
    "    hyperparameters['tokenizer-type'] = '/opt/ml/checkpoints/llama7b-hf'\n",
    "    hyperparameters['output-prefix'] = '/opt/ml/checkpoints/wiki'\n",
    "    hyperparameters[\"checkpoint-dir\"] = '/opt/ml/checkpoints'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "# Need to check if this works on multinode with torchrun.\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"nemo-megatron-data-prep\",\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"process_data_for_megatron.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    # For training with multinode distributed training, set this count. Example: 2\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=2048,\n",
    "    hyperparameters=hyperparameters,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"] if not use_fsx else None,\n",
    "    debugger_hook_config=False,\n",
    "    keep_alive_period_in_seconds=600,\n",
    "\n",
    "    subnets = [SUBNET_ID] if use_fsx else None, # Give SageMaker Training Jobs access to FSx resources in your Amazon VPC\n",
    "    security_group_ids=SEC_GRP if use_fsx else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_fsx:\n",
    "    estimator.fit(data_channels)\n",
    "else:\n",
    "    estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warm pool    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":900})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
