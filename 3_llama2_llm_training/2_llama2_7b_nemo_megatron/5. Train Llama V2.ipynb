{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LLama V2 on Trainium using Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrive the docker image URL stored in step 1\n",
    "%store -r docker_image \n",
    "\n",
    "use_fsx = False # set this to true and check other fsx parameters to use FSxL for the job\n",
    "use_checkpoint = True # set this to True if you ran Notebook 4 and have checkpoint created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:  \n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nemo Megatron uses Hydra based configuration and Yaml config files. In order to support this we will use SageMaker hyperparameters, which will be passed as arguments to the entrypoint script. In the entry point script we will use hydra compose API to read in the passed hyperparameters and override it in config. \n",
    "\n",
    "Note: Please change the trainer.num_nodes parameters accordingly to the number of instances in the estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "NUM_NODES = 2\n",
    "# Trainer parameters\n",
    "hyperparameters[\"trainer.devices\"]=32 \n",
    "hyperparameters[\"trainer.num_nodes\"]=NUM_NODES # Change this to the number of nodes in the training job indicated by instance_count parameter\n",
    "hyperparameters[\"trainer.max_epochs\"]=\"null\"\n",
    "hyperparameters[\"trainer.max_steps\"]=500\n",
    "hyperparameters[\"trainer.val_check_interval\"]=0.99\n",
    "hyperparameters[\"trainer.log_every_n_steps\"]=1\n",
    "hyperparameters[\"trainer.limit_val_batches\"]=1\n",
    "hyperparameters[\"trainer.limit_test_batches\"]=1\n",
    "hyperparameters[\"trainer.accumulate_grad_batches\"]=1\n",
    "hyperparameters[\"trainer.precision\"]=32\n",
    "\n",
    "#Model Parameters for 7b configuration\n",
    "\n",
    "hyperparameters[\"model.micro_batch_size\"]=1\n",
    "hyperparameters[\"model.global_batch_size\"]=256\n",
    "hyperparameters[\"model.tensor_model_parallel_size\"]=8\n",
    "hyperparameters[\"model.pipeline_model_parallel_size\"]=1\n",
    "hyperparameters[\"model.max_position_embeddings\"]=4096\n",
    "hyperparameters[\"model.encoder_seq_length\"]=4096\n",
    "hyperparameters[\"model.hidden_size\"]=4096\n",
    "hyperparameters[\"model.num_layers\"]=32\n",
    "hyperparameters[\"model.num_attention_heads\"]=32\n",
    "hyperparameters[\"model.init_method_std\"]=0.021\n",
    "hyperparameters[\"model.hidden_dropout\"]=0\n",
    "hyperparameters[\"model.layernorm_epsilon\"]=1e-5\n",
    "\n",
    "hyperparameters[\"model.data.num_workers\"]=1\n",
    "hyperparameters[\"model.data.seq_length\"]=4096\n",
    "#hyperparameters[\"model.data.splits_string\"]=\"\\'980,10,10\\'\"\n",
    "hyperparameters[\"model.optim.name\"]=\"adamw\"\n",
    "hyperparameters[\"model.optim.lr\"]=3.0e-4\n",
    "hyperparameters[\"model.optim.betas\"]=\"[0.9,0.95]\"\n",
    "hyperparameters[\"model.optim.weight_decay\"]=0.1\n",
    "hyperparameters[\"model.optim.sched.name\"]=\"CosineAnnealing\"\n",
    "hyperparameters[\"model.optim.sched.warmup_steps\"]=10\n",
    "hyperparameters[\"model.optim.sched.constant_steps\"]=0\n",
    "hyperparameters[\"model.optim.sched.min_lr\"]=3.0e-5\n",
    "hyperparameters[\"model.optim.capturable\"]=True\n",
    "hyperparameters[\"model.sequence_parallel\"]=True\n",
    "hyperparameters[\"model.activations_checkpoint_granularity\"]=\"full\"\n",
    "hyperparameters[\"model.activations_checkpoint_method\"]=\"uniform\"\n",
    "hyperparameters[\"model.activations_checkpoint_num_layers\"]=1\n",
    "hyperparameters[\"model.save_xser\"]=True\n",
    "\n",
    "#experiment manager\n",
    "hyperparameters[\"exp_manager.create_tensorboard_logger\"]=False\n",
    "hyperparameters[\"exp_manager.resume_if_exists\"]=False\n",
    "hyperparameters[\"exp_manager.resume_ignore_no_checkpoint\"]=False\n",
    "hyperparameters[\"exp_manager.create_checkpoint_callback\"]=True\n",
    "\n",
    "hyperparameters[\"exp_manager.checkpoint_callback_params.train_time_interval\"]=36000\n",
    "hyperparameters[\"exp_manager.checkpoint_callback_params.save_last\"]=True\n",
    "hyperparameters[\"model.use_cpu_initialization\"]=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the FSX details from Store Magic \n",
    "\n",
    "if use_fsx:\n",
    "    #retrive fsx details\n",
    "    %store -r fsx_id\n",
    "    %store -r sec_group\n",
    "    %store -r private_subnet_id\n",
    "    %store -r fsx_mount\n",
    "    %store -r fsx_file_system_path\n",
    "else:\n",
    "    use_fsx = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup fsx config for data channels\n",
    "from sagemaker.inputs import FileSystemInput\n",
    "if use_fsx:\n",
    "    FS_ID = fsx_id # FSX ID\n",
    "    FS_BASE_PATH = \"/\" + fsx_mount + \"/\" + fsx_file_system_path # Path in the filesystem that needs to be mounted\n",
    "    SUBNET_ID = private_subnet_id # Subnet to launch SM jobs in\n",
    "    SEC_GRP = [sec_group]\n",
    "\n",
    "    fsx_train_input = FileSystemInput(\n",
    "        file_system_id=FS_ID,\n",
    "        file_system_type='FSxLustre',\n",
    "        directory_path=FS_BASE_PATH + \"/nemo_llama\",\n",
    "        file_system_access_mode=\"rw\"\n",
    "    )\n",
    "    hyperparameters[\"model.tokenizer.type\"]='/opt/ml/input/data/train/llama7b-hf'\n",
    "    hyperparameters[\"model.data.data_prefix\"]=\"[1.0,/opt/ml/input/data/train/wiki_text_document]\"\n",
    "    if use_checkpoint:\n",
    "        hyperparameters[\"model.resume_from_checkpoint\"] = \"/opt/ml/input/data/train/llama7b_weights/mp_rank_07/model_optim_rng.ckpt\"\n",
    "        hyperparameters[\"model.load_xser\"] = True\n",
    "    hyperparameters[\"exp_manager.explicit_log_dir\"]=\"/opt/ml/input/data/train/logs\"\n",
    "    cache_dir = \"/opt/ml/input/data/train/neuron_cache\"\n",
    "    data_channels = {\"train\": fsx_train_input}\n",
    "\n",
    "else:\n",
    "    checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/nemo_llama_experiment\"\n",
    "    # we will use the sagemaker s3 checkpoints mechanism since we need read/write access to the paths.\n",
    "    hyperparameters[\"model.tokenizer.type\"]='/opt/ml/checkpoints/llama7b-hf'\n",
    "    hyperparameters[\"model.data.data_prefix\"]=\"[1.0,/opt/ml/checkpoints/wiki_text_document]\"\n",
    "    if use_checkpoint:\n",
    "        hyperparameters[\"model.resume_from_checkpoint\"] = \"/opt/ml/checkpoints/llama7b_weights/mp_rank_07/model_optim_rng.ckpt\"\n",
    "        hyperparameters[\"model.load_xser\"] = True\n",
    "    hyperparameters[\"exp_manager.explicit_log_dir\"]=\"/opt/ml/model\"\n",
    "    checkpoint_dir = '/opt/ml/checkpoints'\n",
    "    cache_dir = \"/opt/ml/checkpoints/neuron_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "\n",
    "We will launch the training job using Trn1.32xlarge Instance. \n",
    "\n",
    "**_NOTE:_**  When using S3 as data source, initially run the training job with 1 node for few training steps and later stop and increase the number of nodes. We need to do this as the nemo dataset loader creates and stores index files in the checkpoint path when we run the training. This happens on node with rank 0 process and other nodes will read after its done. Since we have checkpoints in S3 , you will get an file not found error when other processes in different node try to access the index files. This is because the syncronization of files from local disk to S3 will not be completed in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'llama-neuron-nemo-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "env = {}\n",
    "\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "env['FI_EFA_FORK_SAFE'] = '1'\n",
    "env['NCCL_SOCKET_IFNAME'] = 'ens'\n",
    "env['XLA_USE_BF16']='1'\n",
    "env['NCCL_SOCKET_IFNAME'] = '^lo,docker'\n",
    "env['NEURON_CC_FLAGS'] = \"--cache_dir=\" + cache_dir\n",
    "\n",
    "# estimator \n",
    "pt_estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    image_uri=docker_image,\n",
    "    instance_count=NUM_NODES,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    job_name=job_name,\n",
    "    environment=env,\n",
    "    disable_output_compression=True,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri if not use_fsx else None,\n",
    "    checkpoint_local_path=checkpoint_dir if not use_fsx else None,\n",
    "    subnets = [SUBNET_ID] if use_fsx else None, # Give SageMaker Training Jobs access to FSx resources in your Amazon VPC\n",
    "    keep_alive_period_in_seconds=600,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_fsx:\n",
    "    pt_estimator.fit(data_channels)\n",
    "else:\n",
    "    pt_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(pt_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
