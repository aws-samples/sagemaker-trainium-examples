{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Llama2 70b SageMaker using AWS Trainium instances\n",
    "\n",
    "This example helps to pretrain a Llama 70b model using NeuronX Distributed on trainium instances. NeuronX Distributed is a package for supporting different distributed training/inference mechanism for Neuron devices. It would provide xla friendly implementations of some of the more popular distributed training/inference techniques. The library can be easily installed via pip.\n",
    "\n",
    "In this notebook, we showcase to pretrain a Llama2 70B model by using the tensor parallel, pipeline parallel, sequence parallel, activation checkpoint as well as constant mask optimization in the neuronx-distributed package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets[s3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and upload data\n",
    "\n",
    "For this example we will download the [wikicorpus data](https://huggingface.co/datasets/wikicorpus) from huggingface datasets , tokenize the data with llama2 tokenizer and then upload it to S3 to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder;\n",
    "access_token = \"hf_xxxx\" # Update the access token to download the tokenizer\n",
    "HfFolder.save_token(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "dataset_name = \"wikicorpus\"\n",
    "dataset_config_name = \"raw_en\"\n",
    "save_path = \"data/wikicorpus_llama2_7B_tokenized_4k\"\n",
    "\n",
    "\n",
    "tokenizer_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "\n",
    "save_path = os.path.expanduser(save_path)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "block_size = 4096\n",
    "\n",
    "raw_datasets = load_dataset(dataset_name, dataset_config_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data using Llama2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "if block_size > tokenizer.model_max_length:\n",
    "    print(\"block_size > tokenizer.model_max_length\")\n",
    "block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:  \n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/neuronx_distributed/data'\n",
    "print(f\"uploading training dataset to: {training_input_path}\")# save train_dataset to s3\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have the data uploaded to S3 and ready to kick start the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training job\n",
    "\n",
    "For the training job we will be using Trn1.32xlarge instances. Each Trn1.32xlarge instances will have 32 neuron cores and we will use Tensor parallelism and pipeline parallelism to shard the model across neuron cores and train. The below cell provides basic setting for pretraining llama 2 70b using Trn1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSES_PER_NODE = 32\n",
    "WORLD_SIZE = 8 # this is the number of nodes in cluster, change this if you wan to tweak the instance_count parameter\n",
    "# Global batch size\n",
    "GBS=512\n",
    "# Input sequence length\n",
    "SEQ_LEN=4096\n",
    "# Pipeline parallel degree\n",
    "PP_DEGREE=8\n",
    "# Tensor parallel degree\n",
    "TP_DEGREE=8\n",
    "# Data paralell size\n",
    "DP=((PROCESSES_PER_NODE * WORLD_SIZE / TP_DEGREE / PP_DEGREE))\n",
    "# Batch size per model replica\n",
    "BS=((GBS / DP))\n",
    "# Number microbatches for pipeline execution\n",
    "# Setting same as BS so each microbatch contains a single datasample\n",
    "NUM_MICROBATCHES=BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"train_batch_size\"] = int(BS)\n",
    "hyperparameters[\"use_meta_device_init\"] = 1\n",
    "hyperparameters[\"training_dir\"] = \"/opt/ml/input/data/train\" # path where sagemaker uploads the training data\n",
    "hyperparameters[\"training_config\"] = \"config.json\" # config file containing llama 70b configuration , change this for tweaking the number of parameters.\n",
    "hyperparameters[\"max_steps\"] = 30000\n",
    "hyperparameters[\"seq_len\"] =SEQ_LEN\n",
    "hyperparameters[\"pipeline_parallel_size\"] = PP_DEGREE\n",
    "hyperparameters[\"tensor_parallel_size\"] = TP_DEGREE\n",
    "hyperparameters[\"num_microbatches\"] = int(NUM_MICROBATCHES)\n",
    "hyperparameters[\"lr\"] = 0.00015\n",
    "hyperparameters[\"min_lr\"] = 1e-05\n",
    "hyperparameters[\"beta1\"] = 0.9\n",
    "hyperparameters[\"beta2\"] = 0.95\n",
    "hyperparameters[\"weight_decay\"] = 0.1\n",
    "hyperparameters[\"warmup_steps\"] = 2000\n",
    "hyperparameters[\"constant_steps\"] = 0\n",
    "hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "hyperparameters[\"tb_dir\"] = \"/opt/ml/checkpoints/tensorboard\" # The tensorboard logs will be stored here and eventually pushed to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/opt/ml/checkpoints/neuron_cache\" # path to neuron cache\n",
    "docker_image = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "# define Training Job Name \n",
    "job_name = f'llama-neuron-nemo-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/nemo_llama_experiment\"\n",
    "checkpoint_dir = '/opt/ml/checkpoints'\n",
    "\n",
    "env = {}\n",
    "\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "env['FI_EFA_FORK_SAFE'] = '1'\n",
    "env['NCCL_SOCKET_IFNAME'] = 'ens'\n",
    "#env['XLA_USE_BF16']='1'\n",
    "env['NEURON_FUSE_SOFTMAX'] = '1'\n",
    "env['MALLOC_ARENA_MAX'] = '128'\n",
    "env['XLA_DOWNCAST_BF16'] = '1'\n",
    "env['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = '5'\n",
    "\n",
    "env['NCCL_SOCKET_IFNAME'] = '^lo,docker'\n",
    "env['NEURON_CC_FLAGS'] = \"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity --cache_dir=\" + cache_dir\n",
    "\n",
    "# estimator \n",
    "pt_estimator = PyTorch(\n",
    "    entry_point='run_llama_nxd.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    image_uri=docker_image,\n",
    "    instance_count=WORLD_SIZE,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    base_job_name=job_name,\n",
    "    environment=env,\n",
    "    input_mode=\"FastFile\",\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600, # this is added to enable warm pool capability\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    checkpoint_local_path=checkpoint_dir,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_estimator.fit({\"train\":training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warmpool \n",
    "\n",
    "Execute the below cell to terminate the warmpool if you no longer need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(pt_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we looked at how to pretrain an llama2 70b model using Amazon SageMaker training jobs on AWS Trainium instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
