{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (OPTIONAL): Convert Neuron X distributed checkpoint to Huggingace format for Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE: This notebook is optional.</b> You should only run this notebook if you are experimenting with the **continuous pretraining** process on Neuronx and previously executed `Notebook 1` and `Notebook 2`. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the training job is saved as [NeuronX checkpoint](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/save_load_developer_guide.html). In this notebook we will convert the neuronX distributed checkpoint into a .pt weights file which can be used for inferencing.\n",
    "\n",
    "To begin with we will retrieve the path for the checkpoint from the model output and also path to Llama 70b config file, this can be retreived from `Notebook 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a `ml.t3.medium instance`. However, to execute the training job for preparing the pre-trained weights for the continuous pre-training process, you may need to request a quota increase. The number of instances you need to request for the quota increase depends on how quickly you may want the training job to complete. The range is between **8** and **32** instances.\n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.trn1.32xlarge` for training job usage\n",
    "   - `ml.trn1.32xlarge` for training warm pool usage\n",
    "   - `Maximum number of instances per training job`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon SageMaker service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The example has the following main sections:\n",
    "\n",
    "1. [Requirements](#Requirements)\n",
    "2. [Setup](#Setup)\n",
    "3. [Training job parameters](#Training-job-parameters)\n",
    "4. [Run training job](#Run-training-job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:</b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select kernel \"<span style=\"color:green;\">Base Python 3.0</span>\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region_name = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the checkpoint s3 uri with the value used in `Notebook 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive the checkpoint s3 directory from Store Magic \n",
    "%store -r checkpoint_s3_uri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'checkpoint_s3_uri' not in vars():\n",
    "    print(\"The variable checkpoint_s3_uri does not exist. Before continuing with this notebook, check the value for checkpoint_s3_uri in Notebook 2 and define the variable within this notebook\")\n",
    "else:\n",
    "    print(checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 checkpoint directory that contains the weights and other relevant data from the continuous pre-training model\n",
    "nxd_checkpoint_path = f\"s3://{checkpoint_s3_uri}/neuronx_llama_experiment/checkpts/step10/model/\" # Checkpoint is saved as part of Notebook 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image for training a models on AWS Trainium\n",
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.18.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about neron docker images:\n",
    "- [AWS Neuron Deep Learning Containers](https://github.com/aws-neuron/deep-learning-containers/tree/main0)\n",
    "- [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the [access token](https://huggingface.co/docs/hub/en/security-tokens) from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"hf_xxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training job parameters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for saving model weights to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"n_layers\"] = 80\n",
    "hyperparameters[\"pp_size\"] = 8\n",
    "hyperparameters[\"tp_size\"] = 8\n",
    "hyperparameters[\"input_dir\"] = \"/opt/ml/input/data/checkpoint\"\n",
    "hyperparameters[\"convert_to_full_model\"] = \"\"\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/model\"\n",
    "hyperparameters[\"access_token\"] = access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training job\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "# NOTES: Multinode with torchrun is a work-in-progresss. Use a single node.\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"neuronx-convert-checkpoint-to-hf\",\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"convert_checkpoints.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=1024,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SageMaker job\n",
    "estimator.fit({\"checkpoint\": nxd_checkpoint_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = estimator.model_data['S3DataSource']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You can find the converted weights here {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
