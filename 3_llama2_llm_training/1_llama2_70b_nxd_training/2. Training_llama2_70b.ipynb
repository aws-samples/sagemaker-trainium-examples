{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training and Continuous Pre-training for Llama2 70b SageMaker using AWS Trainium instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:</b> This notebook can be used for both pretraining and continuous pretraining of large language models.\n",
    "\n",
    "Pretraining: Pretraining pretraining involves training all parameters of a model on a large dataset to improve the model's performance and learning more about of the problem domain.\n",
    "\n",
    "Continuous Pretraining: Continuous pretraining refers to the process of periodically updating the model's parameters with new data during training. This allows the model to adapt and improve over time as it encounters new examples and scenarios.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example provides a walkthrough of pretraining and continuous pretraining for [Llama 70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) model using [NeuronX](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html) Distributed on trainium instances. \n",
    "\n",
    "NeuronX Distributed is a package for supporting different distributed training/inference mechanism for Neuron devices. It would provide xla friendly implementations of some of the more popular distributed training/inference techniques. The library can be easily installed via pip.\n",
    "\n",
    "Running the continuous pre-training process requires to have executed the `Notebook 1` to obtain the path of the `pre-trained weights`. These pre-trained weights represent a checkpoint or a starting point for the continual pretraining process. Continued-pretraining involves taking a pre-trained model (in this case, the pre-trained weights) and further pre-training it on additional to improve its  knowledge, capabilities, generalization across tasks and specific domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This Jupyter Notebook can be run on a `ml.t3.medium instance`. However, to execute the training job for preparing the pre-trained weights for the continuous pre-training process, you may need to request a quota increase. The number of instances you need to request for the quota increase depends on how quickly you may want the training job to complete. The range is between **8** and **32** instances.\n",
    "\n",
    "To request a quota increase, follow these steps:\n",
    "\n",
    "1. Navigate to the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "2. Choose Amazon SageMaker.\n",
    "3. Review your default quota for the following resources:\n",
    "   - `ml.trn1.32xlarge` for training job usage\n",
    "   - `ml.trn1.32xlarge` for training warm pool usage\n",
    "   - `Maximum number of instances per training job`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>NOTE:</b> To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon SageMaker service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The example has the following main sections:\n",
    "\n",
    "1. [Install require packages](#Install-required-packages)\n",
    "2. [Download tokenizer and model](#Download-tokenizer-and-model)\n",
    "3. [Download training dataset](#Download-training-dataset)\n",
    "4. [Tokenize the data using Llama2 tokenizer](#Tokenize-the-data-using-Llama2-tokenizer)\n",
    "5. [Upload data to S3](#Upload-data-to-S3)\n",
    "6. [Run the training job](#Run-the-training-job)\n",
    "6. [Terminate the warmpool](#Terminate-the-warmpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create an Amazon SageMaker Notebook Instance - [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "    - For Notebook Instance type, choose ml.t3.medium.\n",
    "2. For Select Kernel, choose [conda_python3](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html).\n",
    "3. Install the required packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:\n",
    "\n",
    "- </b> For <a href=\"https://aws.amazon.com/sagemaker/studio/\" target=\"_blank\">Amazon SageMaker Studio</a>, select Kernel \"<span style=\"color:green;\">Python 3 (ipykernel)</span>\".\n",
    "\n",
    "- For <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html\" target=\"_blank\">Amazon SageMaker Studio Classic</a>, select Image \"<span style=\"color:green;\">Base Python 3.0</span>\" and Kernel \"<span style=\"color:green;\">Python 3</span>\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this notebook you would need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.218.0 boto3==1.34.97 botocore==1.34.97 --force --quiet\n",
    "!pip install transformers==4.40.1 datasets[s3]==2.19.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "<b>IMPORTANT:</b> You may need to restart the kernel before continuing for the above library installs to be recognized by the remainder of the code! Ignore this message if the libraries were priviously installed in `Notebook 1`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "\n",
    "<b>NOTE:</b>\n",
    "\n",
    "- For enabling the **continous pretraining hyperparameter** for Llama2 70B, set **is_continuous_pretraining** to \"<span style=\"color:green;\">True</span>\"\n",
    "\n",
    "- For enabling the **full pretraining hyperparameters** for Llama2 70B, set **is_continuous_pretraining** to \"<span style=\"color:green;\">False</span>\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_continuous_pretraining = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the [access token](https://huggingface.co/docs/hub/en/security-tokens) to download the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "access_token = \"hf_xxxx\"\n",
    "HfFolder.save_token(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "block_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training dataset\n",
    "\n",
    "For this example we will download the [wikicorpus data](https://huggingface.co/datasets/wikicorpus) from huggingface datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "dataset_name = \"wikicorpus\"\n",
    "dataset_config_name = \"raw_en\"\n",
    "\n",
    "# Create cache directory\n",
    "save_path = \"data/wikicorpus_llama2_7B_tokenized_4k\"\n",
    "save_path = os.path.expanduser(save_path)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Download wikicorpus data\n",
    "raw_datasets = load_dataset(\n",
    "    dataset_name,\n",
    "    dataset_config_name,\n",
    "    cache_dir=save_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data using Llama2 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize training dataset with llama2 tokenizer and then upload it to S3 to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize training dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "if block_size > tokenizer.model_max_length:\n",
    "    print(\"block_size > tokenizer.model_max_length\")\n",
    "block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "# Final training dataset\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region_name = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save training dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/neuronx_distributed/data'\n",
    "print(f\"uploading training dataset to: {training_input_path}\")\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have the data uploaded to S3 and ready to kick start the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training job\n",
    "\n",
    "For the training job we will be using Trn1.32xlarge instances. Each Trn1.32xlarge instances will have 32 neuron cores and we will use Tensor parallelism and pipeline parallelism to shard the model across neuron cores and train. The below cell provides basic setting for pretraining llama 2 70b using Trn1. \n",
    "\n",
    "*Note: Change the number of instances within the cluster to increase or decrease the job execution time. The range is between **8** and **32** `Trn1.32xlarge instances`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of neuron cores per instance\n",
    "PROCESSES_PER_NODE = 32\n",
    "\n",
    "# Number of instances within the cluster, change this if you want to tweak the instance_count parameter\n",
    "WORLD_SIZE = 8\n",
    "\n",
    "# Global batch size\n",
    "GBS = 512\n",
    "\n",
    "# Input sequence length\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# Pipeline parallel degree\n",
    "PP_DEGREE = 8\n",
    "\n",
    "# Tensor parallel degree\n",
    "TP_DEGREE = 8\n",
    "\n",
    "# Data paralell size\n",
    "DP = ((PROCESSES_PER_NODE * WORLD_SIZE / TP_DEGREE / PP_DEGREE))\n",
    "\n",
    "# Batch size per model replica\n",
    "BS = ((GBS / DP))\n",
    "\n",
    "# Number microbatches for pipeline execution. Setting same as BS so each microbatch contains a single datasample\n",
    "NUM_MICROBATCHES = BS\n",
    "\n",
    "# Number of total steps for which to train model. This number should be adjusted to the step number when the loss function is approaching convergence.\n",
    "MAX_STEPS = 1500\n",
    "\n",
    "# Timeout in seconds for training. After this amount of time Amazon SageMaker terminates the job regardless of its current status.\n",
    "MAX_RUN = 2 * (24 * 60 * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for pre-training Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"train_batch_size\"] = int(BS)\n",
    "hyperparameters[\"use_meta_device_init\"] = 1\n",
    "hyperparameters[\"training_dir\"] = \"/opt/ml/input/data/train\" # path where sagemaker uploads the training data\n",
    "hyperparameters[\"training_config\"] = \"config.json\" # config file containing llama 70b configuration , change this for tweaking the number of parameters.\n",
    "hyperparameters[\"max_steps\"] = MAX_STEPS\n",
    "hyperparameters[\"seq_len\"] = SEQ_LEN\n",
    "hyperparameters[\"pipeline_parallel_size\"] = PP_DEGREE\n",
    "hyperparameters[\"tensor_parallel_size\"] = TP_DEGREE\n",
    "hyperparameters[\"num_microbatches\"] = int(NUM_MICROBATCHES)\n",
    "hyperparameters[\"lr\"] = 0.00015\n",
    "hyperparameters[\"min_lr\"] = 1e-05\n",
    "hyperparameters[\"beta1\"] = 0.9\n",
    "hyperparameters[\"beta2\"] = 0.95\n",
    "hyperparameters[\"weight_decay\"] = 0.1\n",
    "hyperparameters[\"warmup_steps\"] = 2000\n",
    "hyperparameters[\"constant_steps\"] = 0\n",
    "hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "hyperparameters[\"tb_dir\"] = \"/opt/ml/checkpoints/tensorboard\" # The tensorboard logs will be stored here and eventually pushed to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for continually pre-training Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if is_continuous_pretraining:\n",
    "    hyperparameters[\"checkpoint_dir\"] = \"/opt/ml/checkpoints/checkpts\"\n",
    "    hyperparameters[\"checkpoint_freq\"] = 10\n",
    "    hyperparameters[\"num_kept_checkpoint\"] = 1\n",
    "    hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "    hyperparameters[\"save_load_xser\"] = 0\n",
    "    hyperparameters[\"pretrained_weight_dir\"] = \"/opt/ml/checkpoints/llama70b_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Docker image for training a models on AWS Trainium\n",
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.18.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about neron docker images:\n",
    "- [AWS Neuron Deep Learning Containers](https://github.com/aws-neuron/deep-learning-containers/tree/main0)\n",
    "- [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the checkpoint s3 uri in Notebook 1\n",
    "%store -r checkpoint_s3_uri "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'checkpoint_s3_uri' not in vars():\n",
    "    print(\"The variable checkpoint_s3_uri does not exist. If you are running the continuous pretraining process, check the value for checkpoint_s3_uri in Notebook 1 and define the variable within this notebook. Otherwise, ignore this message and continue.\")\n",
    "else:\n",
    "    # Define checkpoint directory that will contain the weights and other relevant data for the trained model\n",
    "    checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/neuronx_llama_experiment\"\n",
    "    # Use store magic to save the checkpoint s3 directory to use in subsequent notebooks.\n",
    "    %store checkpoint_s3_uri \n",
    "    print(checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define Training Job Name\n",
    "job_name = f'llama-neuron-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "checkpoint_dir = '/opt/ml/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define neuron chache directory\n",
    "cache_dir = \"/opt/ml/checkpoints/neuron_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Environment variables to be set for use during training job \n",
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "env['FI_EFA_FORK_SAFE'] = '1'\n",
    "env['NEURON_FUSE_SOFTMAX'] = '1'\n",
    "env['MALLOC_ARENA_MAX'] = '128'\n",
    "env['XLA_DOWNCAST_BF16'] = '1'\n",
    "env['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = '5'\n",
    "env['NCCL_SOCKET_IFNAME'] = '^lo,docker'\n",
    "env['NEURON_CC_FLAGS'] = \"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity --cache_dir=\" + cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) for training a job on Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point='run_llama_nxd.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    image_uri=docker_image,\n",
    "    instance_count=WORLD_SIZE,\n",
    "    max_run=MAX_RUN,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    base_job_name=job_name,\n",
    "    environment=env,\n",
    "    input_mode=\"FastFile\",\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600, # this is added to enable warm pool capability\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    checkpoint_local_path=checkpoint_dir,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start training job\n",
    "pt_estimator.fit({\"train\": training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminate the warmpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the below cell to terminate the warmpool if you no longer need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(pt_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we looked at how to pretrain and continually pre-trained Llama2 70b model using Amazon SageMaker training jobs on AWS Trainium instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
