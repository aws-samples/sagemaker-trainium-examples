{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continued pretraining for Llama2 70b SageMaker using AWS Trainium instances\n",
    "\n",
    "This example provides a walkthrough of continued pretraining for [Llama 70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) model using [NeuronX](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/transformers-neuronx/transformers-neuronx-developer-guide.html) Distributed on trainium instances. NeuronX Distributed is a package for supporting different distributed training/inference mechanism for Neuron devices. It would provide xla friendly implementations of some of the more popular distributed training/inference techniques. The library can be easily installed via pip.\n",
    "\n",
    "First, we will obtain the path of the `pre-trained weights` from `Notebook 1`. These pre-trained weights represent a checkpoint or a starting point for the continual pretraining process. Continued-pretraining involves taking a pre-trained model (in this case, the pre-trained weights) and further pre-training it on additional to improve its  knowledge, capabilities, generalization across tasks and specific domains.\n",
    "\n",
    "In this notebook, we showcase continued-pretraining for a Llama2 70B model by using the tensor parallel, pipeline parallel, sequence parallel, activation checkpoint as well as constant mask optimization in the neuronx-distributed package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example has the following main sections:\n",
    "\n",
    "- [Install require packages](#Install-required-packages)\n",
    "- [Download tokenizer and model](#Download-tokenizer-and-model)\n",
    "- [Download training dataset](#Download-training-dataset)\n",
    "- [Tokenize the data using Llama2 tokenizer](#Tokenize-the-data-using-Llama2-tokenizer)\n",
    "- [Upload data to S3](#Upload-data-to-S3)\n",
    "- [Run the training job](#Run-the-training-job)\n",
    "- [Terminate the warmpool](#Terminate-the-warmpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance type quota increase\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "- Open the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "- Choose Amazon SageMaker.\n",
    "- Choose the service quota.\n",
    "- Choose Request quota increase.\n",
    "\n",
    "**Notes**: *To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This Jupyter Notebook can be run on a t3.medium instance (`ml.t3.medium`). However, to fine-tune the `Llama 2 70B` model, we use the `trn1.32xlarge` instance type. Pre-training Llama 2 70b model can run in a `cluster of 8 trn1.32xlarge instances`.* However, we recommend running the training job in a cluster of `32` instances.  \n",
    "\n",
    "*Before you run this notebook, you'll need to request a `quota increase of 32` from Amazon SageMaker for the following resources:*\n",
    "\n",
    "1. *ml.trn1.32xlarge instance type for training job usage*\n",
    "2. *ml.trn1.32xlarge instance type for training warm pool usage*\n",
    "3. *Maximum number of instances per training job*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 --quiet\n",
    "!pip install transformers datasets[s3] --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download tokenizer and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the [access token](https://huggingface.co/docs/hub/en/security-tokens) to download the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub.hf_api import HfFolder\n",
    "access_token = \"hf_xxxx\"\n",
    "HfFolder.save_token(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "block_size = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download training dataset\n",
    "\n",
    "For this example we will download the [wikicorpus data](https://huggingface.co/datasets/wikicorpus) from huggingface datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "# Dataset\n",
    "dataset_name = \"wikicorpus\"\n",
    "dataset_config_name = \"raw_en\"\n",
    "\n",
    "# Create cache directory\n",
    "save_path = \"data/wikicorpus_llama2_7B_tokenized_4k\"\n",
    "save_path = os.path.expanduser(save_path)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Download wikicorpus data\n",
    "raw_datasets = load_dataset(\n",
    "    dataset_name,\n",
    "    dataset_config_name,\n",
    "    cache_dir=save_path,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data using Llama2 tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize training dataset with llama2 tokenizer and then upload it to S3 to use during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize training dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[text_column_name])\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=True,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")\n",
    "\n",
    "if block_size > tokenizer.model_max_length:\n",
    "    print(\"block_size > tokenizer.model_max_length\")\n",
    "block_size = min(block_size, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    load_from_cache_file=True,\n",
    "    desc=f\"Grouping texts in chunks of {block_size}\",\n",
    ")\n",
    "\n",
    "# Final training dataset\n",
    "train_dataset = lm_datasets[\"train\"]\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region_name = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save training dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/neuronx_distributed/data'\n",
    "print(f\"uploading training dataset to: {training_input_path}\")\n",
    "train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"uploaded data to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should have the data uploaded to S3 and ready to kick start the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training job\n",
    "\n",
    "For the training job we will be using Trn1.32xlarge instances. Each Trn1.32xlarge instances will have 32 neuron cores and we will use Tensor parallelism and pipeline parallelism to shard the model across neuron cores and train. The below cell provides basic setting for pretraining llama 2 70b using Trn1. \n",
    "\n",
    "*Note: Change the number of instances within the cluster to increase or decrease the job execution time. You can use down to `8 Trn1.32xlarge instances`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSES_PER_NODE = 32\n",
    "\n",
    "# Number of instances within the cluster\n",
    "WORLD_SIZE = 32 # This is the number of nodes in cluster, change this if you want to tweak the instance_count parameter\n",
    "\n",
    "# Global batch size\n",
    "GBS = 512\n",
    "\n",
    "# Input sequence length\n",
    "SEQ_LEN = 4096\n",
    "\n",
    "# Pipeline parallel degree\n",
    "PP_DEGREE = 8\n",
    "\n",
    "# Tensor parallel degree\n",
    "TP_DEGREE = 8\n",
    "\n",
    "# Data paralell size\n",
    "DP = ((PROCESSES_PER_NODE * WORLD_SIZE / TP_DEGREE / PP_DEGREE))\n",
    "\n",
    "# Batch size per model replica\n",
    "BS = ((GBS / DP))\n",
    "\n",
    "# Number microbatches for pipeline execution\n",
    "# Setting same as BS so each microbatch contains a single datasample\n",
    "NUM_MICROBATCHES = BS\n",
    "\n",
    "# Number of total steps for which to train model.\n",
    "MAX_STEPS = 1500 # This number should be adjusted to the step number when the loss function is approaching convergence.\n",
    "\n",
    "# Timeout in seconds for training\n",
    "MAX_RUN = 2 * (24 * 60 * 60) # After this amount of time Amazon SageMaker terminates the job regardless of its current status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hyperparameters for continually pre-training Llama2 70B model are similar to the ones for pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"train_batch_size\"] = int(BS)\n",
    "hyperparameters[\"use_meta_device_init\"] = 1\n",
    "hyperparameters[\"training_dir\"] = \"/opt/ml/input/data/train\" # path where sagemaker uploads the training data\n",
    "hyperparameters[\"training_config\"] = \"config.json\" # config file containing llama 70b configuration , change this for tweaking the number of parameters.\n",
    "hyperparameters[\"max_steps\"] = MAX_STEPS\n",
    "hyperparameters[\"seq_len\"] = SEQ_LEN\n",
    "hyperparameters[\"pipeline_parallel_size\"] = PP_DEGREE\n",
    "hyperparameters[\"tensor_parallel_size\"] = TP_DEGREE\n",
    "hyperparameters[\"num_microbatches\"] = int(NUM_MICROBATCHES)\n",
    "hyperparameters[\"lr\"] = 0.00015\n",
    "hyperparameters[\"min_lr\"] = 1e-05\n",
    "hyperparameters[\"beta1\"] = 0.9\n",
    "hyperparameters[\"beta2\"] = 0.95\n",
    "hyperparameters[\"weight_decay\"] = 0.1\n",
    "hyperparameters[\"warmup_steps\"] = 2000\n",
    "hyperparameters[\"constant_steps\"] = 0\n",
    "hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "hyperparameters[\"tb_dir\"] = \"/opt/ml/checkpoints/tensorboard\" # The tensorboard logs will be stored here and eventually pushed to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for continually pre-training Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters[\"checkpoint_dir\"] = \"/opt/ml/checkpoints/checkpts\"\n",
    "hyperparameters[\"checkpoint_freq\"] = 10\n",
    "hyperparameters[\"num_kept_checkpoint\"] = 1\n",
    "hyperparameters[\"use_zero1_optimizer\"] = 1\n",
    "hyperparameters[\"save_load_xser\"] = 0\n",
    "hyperparameters[\"pretrained_weight_dir\"] = \"/opt/ml/checkpoints/llama70b_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image for training a models on AWS Trainium\n",
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.15.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about neron docker images:\n",
    "- [AWS Neuron Deep Learning Containers](https://github.com/aws-neuron/deep-learning-containers/tree/main0)\n",
    "- [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Define Training Job Name \n",
    "job_name = f'llama-neuron-nemo-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# Define checkpoint directory that contains the weights and other relevant data for the trained model\n",
    "checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/nemo_llama_experiment\"\n",
    "checkpoint_dir = '/opt/ml/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neuron chache directory\n",
    "cache_dir = \"/opt/ml/checkpoints/neuron_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variables to be set for use during training job \n",
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'\n",
    "env['FI_EFA_FORK_SAFE'] = '1'\n",
    "env['NCCL_SOCKET_IFNAME'] = 'ens'\n",
    "#env['XLA_USE_BF16']='1'\n",
    "env['NEURON_FUSE_SOFTMAX'] = '1'\n",
    "env['MALLOC_ARENA_MAX'] = '128'\n",
    "env['XLA_DOWNCAST_BF16'] = '1'\n",
    "env['NEURON_RT_ASYNC_EXEC_MAX_INFLIGHT_REQUESTS'] = '5'\n",
    "env['NCCL_SOCKET_IFNAME'] = '^lo,docker'\n",
    "env['NEURON_CC_FLAGS'] = \"--model-type=transformer --distribution-strategy=llm-training --enable-saturate-infinity --cache_dir=\" + cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) for training a job on Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "pt_estimator = PyTorch(\n",
    "    entry_point='run_llama_nxd.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    image_uri=docker_image,\n",
    "    instance_count=WORLD_SIZE,\n",
    "    max_run=MAX_RUN,\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=role,\n",
    "    base_job_name=job_name,\n",
    "    environment=env,\n",
    "    input_mode=\"FastFile\",\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600, # this is added to enable warm pool capability\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    checkpoint_local_path=checkpoint_dir,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} # enable torchrun \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training job\n",
    "pt_estimator.fit({\"train\": training_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminate the warmpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the below cell to terminate the warmpool if you no longer need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.update_training_job(pt_estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we looked at how to continually pre-trained Llama2 70b model using Amazon SageMaker training jobs on AWS Trainium instance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
