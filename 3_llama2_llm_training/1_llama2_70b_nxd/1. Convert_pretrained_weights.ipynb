{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert pre-trained weights with tensor parallelism for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start the fine-tuning process, we need to download the pre-trained weights for the [Llama 70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) model. In this notebook, we'll be using a combination of two parallelism techniques: [Pipeline Parallelism and Tensor Parallelism](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-extended-features-pytorch-ranking-mechanism.html). By leveraging these techniques, we can convert the pre-trained weights into a .pt (PyTorch) weights file. This converted weights file will then be used for fine-tuning the model in `Notebook 2`.\n",
    "\n",
    "Pipeline Parallelism is a technique that divides a deep neural network into multiple stages or layers, where each stage is executed on a different device (e.g., GPU). This approach allows for efficient use of computational resources by distributing the workload across multiple devices.\n",
    "\n",
    "Tensor Parallelism, on the other hand, splits the tensors (multidimensional arrays) of the neural network across multiple devices. This technique is particularly useful for models with large tensors that cannot fit into the memory of a single device.\n",
    "\n",
    "By combining these two parallelism techniques, we can effectively handle the large size of the Llama 70b model and convert its pre-trained weights into a more efficient and usable format (.pt) for the fine-tuning process in Notebook 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The example has the following main sections:\n",
    "\n",
    "- [Install require packages](#Install-required-packages)\n",
    "- [Download and prepare pre-trained weights for fine-tuning](#Download-and-prepare-pre-trained-weights-for-fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance type quota increase\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "- Open the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "- Choose Amazon SageMaker.\n",
    "- Choose the service quota.\n",
    "- Choose Request quota increase.\n",
    "\n",
    "**Notes**: *To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "*This Jupyter Notebook can be run on a t3.medium instance (`ml.t3.medium`). However, to save the pre-trained weights into a .pt weights file, we use a `trn1.32xlarge` instance type.*\n",
    "\n",
    "*Before you run this notebook, you'll need to request a `quota increase of 32` from Amazon SageMaker for the following resources:*\n",
    "\n",
    "1. *ml.trn1.32xlarge instance type for training job usage*\n",
    "2. *ml.trn1.32xlarge instance type for training warm pool usage*\n",
    "3. *Maximum number of instances per training job*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and prepare pre-trained weights for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region_name = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the [access token](https://huggingface.co/docs/hub/en/security-tokens) to download the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"hf_xxxx\"\n",
    "model_name = \"meta-llama/Llama-2-70b-chat-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hyperparameters for converting pre-trained weights for Llama2 70B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"access_token\"] =  access_token\n",
    "hyperparameters[\"model_name\"] = model_name\n",
    "hyperparameters[\"tp_size\"] = 8\n",
    "hyperparameters[\"pp_size\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sagemaker s3 checkpoints mechanism since we need read/write access to the paths.\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/checkpoints/llama70b_weights\"\n",
    "hyperparameters[\"checkpoint-dir\"] = '/opt/ml/checkpoints'\n",
    "hyperparameters[\"n_layers\"] = 80\n",
    "hyperparameters[\"convert_from_full_model\"] = \"\" #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Docker image for training a models on AWS Trainium\n",
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.17.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about neron docker images:\n",
    "- [AWS Neuron Deep Learning Containers](https://github.com/aws-neuron/deep-learning-containers/tree/main0)\n",
    "- [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint directory that contains the weights and other relevant data for the trained model\n",
    "checkpoint_s3_uri = \"s3://\" + sagemaker_session_bucket + \"/neuronx_llama_experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch estimator](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) for running a job on Amazon SageMaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "# NOTES: Multinode with torchrun is a work-in-progresss. Use a single node.\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"neuronx-llama-download-model-weights\",\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"convert_checkpoints.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=1024,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    checkpoint_local_path=hyperparameters[\"checkpoint-dir\"],\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start SageMaker job\n",
    "estimator.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
