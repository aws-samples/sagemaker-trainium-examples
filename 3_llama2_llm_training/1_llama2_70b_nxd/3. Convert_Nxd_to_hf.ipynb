{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Neuron X distributed checkpoint to Huggingace format for Inferencing\n",
    "\n",
    "The output from the training job is saved as [NeuronX checkpoint](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/neuronx-distributed/save_load_developer_guide.html). In this notebook we will convert the neuronX distributed checkpoint into a .pt weights file which can be used for inferencing.\n",
    "\n",
    "To begin with we will retrieve the path for the checkpoint from the model output and also path to Llama 70b config file, this can be retreived from `Notebook 2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example has the following main sections:\n",
    "\n",
    "- [Install require packages](#Install-required-packages)\n",
    "- [Convert Neuron X checkpoints to HF format](#Convert-Neuron-X-checkpoints-to-HF-format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance type quota increase\n",
    "\n",
    "Complete the following steps:\n",
    "\n",
    "- Open the [Service Quotas console](https://console.aws.amazon.com/servicequotas/).\n",
    "- Choose Amazon EC2.\n",
    "- Choose the service quota.\n",
    "- Choose Request quota increase.\n",
    "\n",
    "**Notes**: *To make sure that you have enough quotas to support your usage requirements, it's a best practice to monitor and manage your service quotas. Requests for Amazon EC2 service quota increases are subject to review by AWS engineering teams. Also, service quota increase requests aren't immediately processed when you submit a request. After your request is processed, you receive an email notification.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This Jupyter Notebook can be run on a t3.medium instance (`ml.t3.medium`). However, to save the pre-trained weights into a .pt weights file, we use a `trn1.32xlarge` instance type.*\n",
    "\n",
    "*Before you run this notebook, you'll need to request a `quota increase of 32` from Amazon SageMaker for the following resources:*\n",
    "\n",
    "1. *ml.trn1.32xlarge instance type for training job usage*\n",
    "\n",
    "2. *ml.trn1.32xlarge instance type for training warm pool usage*\n",
    "\n",
    "3. *Maximum number of instances per training job*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker boto3 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Neuron X checkpoints to HF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker \n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "region_name = sess.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the [access token](https://huggingface.co/docs/hub/en/security-tokens) from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"hf_xxxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the checkpoint s3 uri with the value used in `Notebook 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 checkpoint directory that contains the weights and other relevant data from the fine-tuned model\n",
    "checkpoint_s3_uri = \"<fine-tuning-checkpoint-s3-uri>\"\n",
    "nxd_checkpoint_path = f\"s3://{checkpoint_s3_uri}/neuronx_llama_experiment/checkpts/step10/model/\" # Checkpoint is saved as part of Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for saving model weights to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "hyperparameters[\"n_layers\"] = 80\n",
    "hyperparameters[\"pp_size\"] = 8\n",
    "hyperparameters[\"tp_size\"] = 8\n",
    "hyperparameters[\"input_dir\"] = \"/opt/ml/input/data/checkpoint\"\n",
    "hyperparameters[\"convert_to_full_model\"] = \"\"\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/model\"\n",
    "hyperparameters[\"access_token\"] = access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker image for training a models on AWS Trainium\n",
    "docker_image = f\"763104351884.dkr.ecr.{region_name}.amazonaws.com/pytorch-training-neuronx:1.13.1-neuronx-py310-sdk2.17.0-ubuntu20.04\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about neron docker images:\n",
    "- [AWS Neuron Deep Learning Containers](https://github.com/aws-neuron/deep-learning-containers/tree/main0)\n",
    "- [Available Deep Learning Containers Images](https://github.com/aws/deep-learning-containers/blob/master/available_images.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Handle end-to-end Amazon SageMaker training and deployment tasks.\n",
    "# NOTES: Multinode with torchrun is a work-in-progresss. Use a single node.\n",
    "estimator = PyTorch(\n",
    "    base_job_name=\"neuronx-convert-checkpoint-to-hf\",\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"convert_checkpoints.py\",\n",
    "    role=role,\n",
    "    image_uri=docker_image,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.trn1.32xlarge\",\n",
    "    sagemaker_session=sess,\n",
    "    volume_size=1024,\n",
    "    hyperparameters=hyperparameters,\n",
    "    debugger_hook_config=False,\n",
    "    disable_output_compression=True,\n",
    "    keep_alive_period_in_seconds=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start SageMaker job\n",
    "estimator.fit({\"checkpoint\": nxd_checkpoint_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = estimator.model_data['S3DataSource']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"You can find the converted weights here {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
