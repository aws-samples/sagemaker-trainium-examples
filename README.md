## Amazon SageMaker and AWS Trainium Examples

This repository contains examples for running Deep Learning training jobs using AWS Trainium instances and Amazon SageMaker. 

### Repository contents

1. [Text classification using Transformers](https://github.com/aws-samples/sagemaker-trainium-examples/blob/main/1_text_classification/Fine%20tune%20Transformers%20for%20building%20classification%20models%20using%20SageMaker%20and%20Trainium.ipynb) - This example shows how we can train and deploy a BERT based classification model which use Amazon Polarity dataset.

2. [Pretrain BERT using Wiki Data](https://github.com/aws-samples/sagemaker-trainium-examples/blob/main/2_pretraining_bert/Pretrain%20BERT%20Model%20using%20Wiki%20Dataset.ipynb) - The example can be used to Pretrain BERT model using AWS Trainium. We will use wiki data to run the pretraining.

3. [Pretrain Llama 2 70b using Wikicorpus](https://github.com/aws-samples/sagemaker-trainium-examples/blob/main/3_llama2_llm_training/1_llama2_70b_nxd_training/2.%20Training_llama2_70b.ipynb) - The example helps to pretrain llama2 models using NeuronX distributed library.

4. [Continual Pretraining of Llama 2 70b using Wikicorpus](https://github.com/aws-samples/sagemaker-trainium-examples/blob/main/3_llama2_llm_training/1_llama2_70b_nxd_training/1.%20(Optional)%20Convert_pretrained_weights.ipynb) - The example helps to continuous pretrain llama2 models using NeuronX distributed library.

5. [Pretrain/Fine tune Llama using Wiki Data](https://github.com/aws-samples/sagemaker-trainium-examples/tree/main/3_llama2_llm_training/2_llama2_7b_nemo_megatron) - The example can be used to pretrain/fine tune llama2 7b model using Neuronx nemo megatron library.



## Security

See [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.

## License

This library is licensed under the MIT-0 License. See the LICENSE file.

